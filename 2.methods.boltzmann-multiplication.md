# Boltzmann multiplication

Boltzmann multiplication calculates the ensemble preferences by multiplying for each action the action-selection probabilities given by the RL algorithms:

$$ p_t(s_t, a[i]) = \prod_j \pi_t^j (s_t, a[i]) $$

$\pi_t^j(s_t, a[i])$ is the policy for algorithm j at time t for state $s_t$ and action $a[i]$.
Since all the RL algorithms use Boltzmann exploration, 
preference values are never zero.
This is important since if only one RL algorithm return zero,
Boltzmann multiplication would result for a zero probability for that action,
irrespectively of that the other algorithms return high or low probabilities.
The ensemble policy for actions selection is calculated in the following way:

$$ \pi_t (s_t, a[i]) = \frac{ p_t(s_t, a[i])^{\frac{1}{\tau}}}{\sum_k p_t(s_t, a[k])^{\frac{ 1 }{\tau}}} $$

<!---

wiering 2008

Boltzmann Multiplication. Another possibility is multiply-
ing all the action selection probabilities for each action based
on the policies of the algorithms. The preference values of the
ensemble are:

...

A potential problem with this method is that one algorithm
can set the preference values of any number of actions to zero
when it has a zero probability of choosing those actions. Since
all our algorithms use Boltzmann exploration, this was not an
issue in our experiments.

--->