# ACLA (Actor-Critic Learning Automaton)

Actor-Critic Learning Automaton (ACLA)(wiering 2008[6]) is an on-policy learning algorithm that learns a state value-function V and Actor function P.
After each experience 

wiering 2008 $(s_t, a_t, r_t, s_{t+1})$,
the state value-function is updated in the same way as Actor-Critic or QV-learning (wiering 2007):

$$ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $$

$$ V(s_t) := V(s_t) + \beta ( \delta_t ) $$

The actor function maps states to preferences for actions
and is updated by an automaton-like updating rule (wiering 2008 [20])
The policy mapping update depends on the sign of $\delta$:

$\delta_t \geq 0$

$$ a = a_t: P(s_t, a_t) := P(s_t, a_t) + \alpha ( 1 - P(s_t, a_t)) $$

$$ \forall a \neq a_t:  P(s_T, a) := P(s_t, a) + \alpha ( 0 - P(s_t, a))$$


$\delta_t < 0$

$$ a = a_t: P(s_t, a_t) := P(s_t, a_t) + \alpha ( 0 - P(s_t, a_t)) $$

$$ \forall a \neq a_t:  P(s_t, a) := P(s_t, a) + \alpha \left( \frac{P(s_t, a)}{\sum_{b \neq a_t}P(s_t, b)} - P(s_t, a) \right)$$

There are also additional rules to ensure that the target values are between 0 and 1 and existing.
If $P(s_t,a)$ is greater than 1, the value gets changed to 1.
If $P(s_t,a)$ is smaller than 0, the value gets changed to zero.
If the denominator is 0, the new value equals $\frac{1}{|A| -1}$,
with $|A|$ being the number of actions (wiering 2008).

<!---

wiering 2007


Abstract— This paper describes two novel on-policy reinforce-
ment learning algorithms, named QV(λ)-learning and the actor
critic learning automaton (ACLA). Both algorithms learn a state
value-function using TD(λ)-methods. The difference between the
algorithms is that QV-learning uses the learned value function
and a form of Q-learning to learn Q-values, whereas ACLA uses
the value function and a learning automaton-like update rule to
update the actor. We describe several possible advantages of these
methods compared to other value-function-based reinforcement
learning algorithms such as Q-learning, Sarsa, and conventional
Actor-Critic methods. Experiments are performed on (1) small,
(2) large, (3) partially observable, and (4) dynamic maze problems
with tabular and neural network value-function representations,
and on the mountain car problem. The overall results show
that the two novel algorithms can outperform previously known
reinforcement learning algorithms.

Actor Critic Learning Automaton. ACLA learns a state
value-function in the same way as QV-learning, but ACLA
uses a learning automaton-like update rule [5] for changing
the policy mapping states to probabilities (or preferences) for
actions. The updates after an experience (s t , a t , r t , s t+1 ) of
ACLA are the following:

V (s t ) := V (s t ) + β(r t + γV (s t+1 ) − V (s t ))

and, now we use an update rule that examines whether the last
performed action was good (in which case the state-value was
increased) or not. We do this with the following update rule:

if ...
else ...

After which we add ΔP (s t , a) to P (s t , a). For ACLA we used
some additional rules to ensure the targets are always between
0 and 1, independent of the initialization. This is done by using
1 if the target is larger than 1, and 0 if the target is smaller
than 0. If the denominator ≤ 0, all targets in the last part of
1
where |A| is the number of
the update rule get the value |A|−1
actions. The update in case of δ t < 0 is chosen to increase the
preference of actions which are good more than actions that
are considered worse. Above is the ACLA− algorithm, we
also extended ACLA− to ACLA+ which can make multiple
updates relying on the size of δ t = γV (s t+1 ) + r t − V (s t ).

--->

<!---

wiering 2008

ACLA. The Actor Critic Learning Automaton (ACLA) [6]
learns a state value-function in the same way as AC and QV-
learning, but ACLA uses a learning automaton-like update rule
[20] for changing the policy mapping states to probabilities
(or preferences) for actions. The updates after an experience
(s t , a t , r t , s t+1 ) of ACLA are the use of Equation 1, and
now we use an update rule that examines whether the last
performed action was good (in which case the state-value was
increased) or not. We do this with the following update rule:

If δ t ≥ 0
∆P (s t , a t ) = α(1 − P (s t , a t )) and
∀a 6 = a t ∆P (s t , a) = α(0 − P (s t , a))

Else
∆P (s t , a t ) = α(0 − P (s t , a t )) and
...

where δ t = γV (s t+1 ) + r t − V (s t ), and ∆P (s, a) is added to
P (s, a). ACLA uses some additional rules to ensure the targets
are always between 0 and 1, independent of the initialization
(e.g. of neural network weights). This is done by using 1 if
the target is larger than 1, and 0 if the target is smaller than
0. If the denominator is less than or equal to 0, all targets
1
in the last part of the update rule get the value |A|−1
where
|A| is the number of actions. ACLA was shown to outperform
Q-learning and Sarsa on a number of problems when ǫ-greedy
exploration was used [6].

--->