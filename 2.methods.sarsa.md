
# SARSA

Like Q-learning, 
SARSA is temporal difference (TD) reinforcement learning algorithm that learns the action-value Q-function (wiering 2007). 
Unlike Q-learning,
SARSA is on-policy,
meaning that the approximation of optimal $Q^*$ values depend on the policy being followed (wiering 2007 [6]).
An experience is defined as the quintuple $(s_t, a_t, r_t, s_{t+1}, a_{t+1})$.
This quintuple is what gave rise to the name SARSA (sutton 2018).
After every transition, Q-values are updated by the following method:

$$Q(s_t,a_T) := Q(s_t,a_t) + \alpha ( r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t) )$$

As with tabular Q-learning, tabular SARSA converges towards $Q^*$ if all state-action pairs are visited an infinite number of times.


<!---

wiering 2007

Sarsa. Instead of Q-learning, we can also use the on-policy
algorithm Sarsa [6], [10] for learning Q-values. Sarsa makes
the following update after an experience (s t , a t , r t , s t+1 , a t+1 ):

Q(s t , a t ) := Q(s t , a t ) + α(r t + γQ(s t+1 , a t+1 ) − Q(s t , a t ))

Tabular Sarsa converges in the limit to the optimal policy under
proper learning rate annealing if the exploration policy is GLIE
(greedy in the limit with infinite exploration), which means
that the agent should always explore, but stop exploring after
an infinite number of steps [8].
--->

<!---

sutton 2018

We turn now to the use of TD prediction methods for the control problem. As usual, we
follow the pattern of generalized policy iteration (GPI), only this time using TD methods
for the evaluation or prediction part. As with Monte Carlo methods, we face the need to
trade o↵ exploration and exploitation, and again approaches fall into two main classes:
on-policy and o↵-policy. In this section we present an on-policy TD control method.
The first step is to learn an action-value function rather than a state-value function.
In particular, for an on-policy method we must estimate q ⇡ (s, a) for the current behavior
policy ⇡ and for all states s and actions a. This can be done using essentially the same TD
method described above for learning v ⇡ . Recall that an episode consists of an alternating
sequence of states and state–action pairs:

In the previous section we considered transitions from state to state and learned the
values of states. Now we consider transitions from state–action pair to state–action pair,
and learn the values of state–action pairs. Formally these cases are identical: they are
both Markov chains with a reward process. The theorems assuring the convergence of
state values under TD(0) also apply to the corresponding algorithm for action values:

This update is done after every transition from a nonterminal state S t . If
S t+1 is terminal, then Q(S t+1 , A t+1 ) is defined as zero. This rule uses every
element of the quintuple of events, (S t , A t , R t+1 , S t+1 , A t+1 ), that make up a
transition from one state–action pair to the next. This quintuple gives rise to
the name Sarsa for the algorithm. The backup diagram for Sarsa is as shown
Sarsa
to the right.
It is straightforward to design an on-policy control algorithm based on the Sarsa
prediction method. As in all on-policy methods, we continually estimate q ⇡ for the
behavior policy ⇡, and at the same time change ⇡ toward greediness with respect to q ⇡ .
The general form of the Sarsa control algorithm is given in the box on the next page.
The convergence properties of the Sarsa algorithm depend on the nature of the policy’s
dependence on Q. For example, one could use "-greedy or "-soft policies. Sarsa converges
with probability 1 to an optimal policy and action-value function as long as all state–action
pairs are visited an infinite number of times and the policy converges in the limit to
the greedy policy (which can be arranged, for example, with "-greedy policies by setting
" = 1/t).

--->