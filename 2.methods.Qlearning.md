# Q-learning


###
wiering 2007
###
A famous algorithm for learning a Q-function
is Q-learning [12], [13]. Q-learning makes an update after an
experience (s t , a t , r t , s t+1 ) as follows:

Q(s t , a t ) := Q(s t , a t ) + α(r t + γ max Q(s t+1 , a) − Q(s t , a t ))

Where 0 ≤ α ≤ 1 is the learning rate. Q-learning is an off-
policy reinforcement learning algorithm [11], which means
that the agent learns about the optimal value-function while
following another behavioral policy that includes exploration
steps. This has as advantage that it does not matter how
much exploration is used, as long as the agent visits all state-
action pairs an infinite number of times, tabular Q-learning
(with appropriate learning rate adaptation) will converge to the
optimal Q-function [13]. A disadvantage of Q-learning is that
it can diverge when combined with function approximators.
Another possible disadvantage is that off-policy algorithms do
not modify the behavior of the agent to better deal with the
exploration/exploitation dilemma [8].
###


###
sutton 2018
###
One of the early breakthroughs in reinforcement learning was the development of an
o↵-policy TD control algorithm known as Q-learning (Watkins, 1989), defined by

Q(S t , A t )hQ(S t , A t ) + ↵ R t+1 + max Q(S t+1 , a)a i

In this case, the learned action-value function, Q, directly approximates q ⇤ , the optimal
action-value function, independent of the policy being followed. This dramatically
simplifies the analysis of the algorithm and enabled early convergence proofs. The policy
still has an e↵ect in that it determines which state–action pairs are visited and updated.
However, all that is required for correct convergence is that all pairs continue to be
updated. As we observed in Chapter 5, this is a minimal requirement in the sense that
any method guaranteed to find optimal behavior in the general case must require it.
Under this assumption and a variant of the usual stochastic approximation conditions on
the sequence of step-size parameters, Q has been shown to converge with probability 1 to
q ⇤ . The Q-learning algorithm is shown below in procedural form.
What is the backup diagram for Q-learning? The rule (6.8) updates a state–action
pair, so the top node, the root of the update, must be a small, filled action node. The
update is also from action nodes, maximizing over all those actions possible in the next
state. Thus the bottom nodes of the backup diagram should be all these action nodes.
Finally, remember that we indicate taking the maximum of these “next action” nodes
with an arc across them (Figure 3.4-right). Can you guess now what the diagram is? If
so, please do make a guess before turning to the answer in Figure 6.4 on page 134.
###
