# Majority voting

Each of the n RL algorithms defines what it thinks to be the best action.
Majority voting will transform these best actions into preferences in the following way:

$$ p_t(s_t, a[i]) = \sum_{j=1}^{n} I(a[i],a_t^j)$$

with $a_t^j$ being the best action according to algorithm j at time t and

$$ x=y: I(x,y) = 1 $$

$$ x \neq y: I(x,y) = 0 $$

The following Boltzmann distribution based ensemble policy is used for actions selection:

$$ \pi_t (s_t, a[i]) = \frac{exp[\frac{p_t(s_t, a[i])}{\tau}]}{\sum_k exp[\frac{p_t(s_t, a[k])}{\tau}]} $$

This policy makes sure that the most probable action is the best action according to most algorithms, but also ensures exploration (wiering 2008).


<!---

wiering 2008

Majority Voting. The preference values calculated by the
majority voting ensemble using n different RL algorithms are

p t (s t , a[i]) =
n
X
I(a[i], a jt )

where I(x, y) is the indicator function that outputs 1 when
x = y and 0 otherwise. The most probable action is simply
the action that is most often the best action according to
the algorithms. This method resembles a bagging ensemble
method for combining classifiers with majority voting, with
the big difference that because of exploration we do not always
select the action which is preferred by most algorithms.

--->