# Rank voting

Preference values of the ensemble are give by:

$$ p_t(s_t, a[i]) = \sum_{j=1}^{n} r_t^j(a[i]) $$

If m actions are possible in state $s_t$,
$r_t^j(a[1])$, $r_t^j(a[2])$, ... , $r_t^j(a[m])$ 
denotes the weights for these actions as determined by RL algorithm j.
The most probable action is weighted m times, the second best m-1 times, and so on...
(wiering 2008).
As with majority voting, 
the rank voting algorithm uses the following Boltzmann distribution based ensemble policy to ensure both exploitation and exploration:

$$ \pi_t (s_t, a[i]) = \frac{exp[\frac{p_t(s_t, a[i])}{\tau}]}{\sum_k exp[\frac{p_t(s_t, a[k])}{\tau}]} $$

<!---

wiering 2008

Rank Voting. Let r t j (a[1]), . . . , r t j (a[m]) denote the weights
according to the ranks of the action selection probabilities,
such that if π t j (a[i]) ≥ π t j (a[k]) then r t j (a[i]) ≥ r t j (a[k]). For
example, the most probable action could be weighted m times,
the second most probable m − 1 times and so on. This is the
weighting we used in our experiments. The preference values
of the ensemble are:

...

--->