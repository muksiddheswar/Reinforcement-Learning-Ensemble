<!---

wiering 2007


Abstract— This paper describes two novel on-policy reinforce-
ment learning algorithms, named QV(λ)-learning and the actor
critic learning automaton (ACLA). Both algorithms learn a state
value-function using TD(λ)-methods. The difference between the
algorithms is that QV-learning uses the learned value function
and a form of Q-learning to learn Q-values, whereas ACLA uses
the value function and a learning automaton-like update rule to
update the actor. We describe several possible advantages of these
methods compared to other value-function-based reinforcement
learning algorithms such as Q-learning, Sarsa, and conventional
Actor-Critic methods. Experiments are performed on (1) small,
(2) large, (3) partially observable, and (4) dynamic maze problems
with tabular and neural network value-function representations,
and on the mountain car problem. The overall results show
that the two novel algorithms can outperform previously known
reinforcement learning algorithms.

--->