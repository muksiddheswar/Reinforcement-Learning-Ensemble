# Q-learning

Q-learning is an off-policy, temporal difference (TD) reinforcement learning algorithm that learns by updating a Q-function.
This action-value function Q directly approximates the optimal action-value function $Q^*$ and it does this independent of the policy that is being followed. 
Q-learning updates with new experience $(s_t, a_t, r_t, s_{t+1})$ in the following way:

$$Q(s_t,a_T) := Q(s_t,a_t) + \alpha ( r_t + \gamma max_a Q(s_{t+1}, a) - Q(s_t,a_t) )$$

With $0 \leq \alpha \leq 1$ being the learning rate, 
and $0 \leq \gamma \leq 1$ being the discount factor.
Higher $\gamma$ values will result in the agent taking into account not only the immediate reward of an action, but also future rewards that will be available in future states as a consequence of this action.
The advantage of tabular Q-learning is that it will always converge to $Q^*$.
It does not matter what behavioral policy is used, 
as long as  each state-action pair is visited an infinite number of times (wiering 2007 [13]).
However, Q-learning combined with function approximators, such as neural networks, have been observed to diverge (wiering 2007).