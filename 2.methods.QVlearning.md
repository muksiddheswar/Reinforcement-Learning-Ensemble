# QV-learning

QV-learning (wiering 2008[6]) is very similar to Actor-Critic.
It is also an on-policy learning method that learns a state value-function V with TD method, 
and an Actor function to map the states for each action to preference values.
However, QV-learning learns actual Q-values as preference values (wiering 2008).
After each experience $(s_t, a_t, r_t, s_{t+1})$,
the V value-function is updates in the same way as in the Actor-Critic method (wiering 2007):

$$ V(s_t) := V(s_t) + \beta ( r_t + \gamma V(s_{t+1}) - V(s_t) )   $$

The update for the Q-function is similar to the Actor update,
but the $- V(s_t)$ at the end is replaced by $- Q(s_t, a_t)$:

$$ Qvalue: Q(s_t, a_t) := Q(s_t, a_t) + \alpha ( r_t + \gamma V(s_{t+1}) - Q(s_t, a_t))$$



<!---

wiering 2008

QV-learning. QV-learning [6] works by keeping track of
both the Q- and V-functions. In QV-learning the state value-
function V is learned with TD-methods [19]. This is similar
to Actor-Critic methods. The new idea is that the Q-values
simply learn from the V-values using the one-step Q-learning
algorithm. In contrast to AC these learned values can be seen
as actual Q-values and not as preference values. The updates
after an experience (s t , a t , r t , s t+1 ) of QV-learning are the use
of Equation 1 and:

Q(s t , a t ) := Q(s t , a t ) + α(r t + γV (s t+1 ) − Q(s t , a t ))

Note that the V-value used in this second update rule is learned methods have been used for combining function approximators
by QV-learning and not defined in terms of Q-values. There is to store the value function [14], [15], [16], [17], and this can
a strong resemblance with the Actor-Critic method; the only be an efficient way for improving an RL algorithm. In contrast
difference is the second learning rule where V (s t ) is replaced to previous research, we combine different RL algorithms that
by Q(s t , a t ) in QV-learning.

--->

<!---

wiering 2007


Abstract— This paper describes two novel on-policy reinforce-
ment learning algorithms, named QV(λ)-learning and the actor
critic learning automaton (ACLA). Both algorithms learn a state
value-function using TD(λ)-methods. The difference between the
algorithms is that QV-learning uses the learned value function
and a form of Q-learning to learn Q-values, whereas ACLA uses
the value function and a learning automaton-like update rule to
update the actor. We describe several possible advantages of these
methods compared to other value-function-based reinforcement
learning algorithms such as Q-learning, Sarsa, and conventional
Actor-Critic methods. Experiments are performed on (1) small,
(2) large, (3) partially observable, and (4) dynamic maze problems
with tabular and neural network value-function representations,
and on the mountain car problem. The overall results show
that the two novel algorithms can outperform previously known
reinforcement learning algorithms.

QV-learning. The updates after an experience
(s t , a t , r t , s t+1 ) of QV-learning are the following:

V (s t ) := V (s t ) + β(r t + γV (s t+1 ) − V (s t ))

Q(s t , a t ) := Q(s t , a t ) + α(r t + γV (s t+1 ) − Q(s t , a t ))

Note that the V-value used in this second update rule is learned
by QV-learning and not defined in terms of Q-values. There is
a strong resemblance with the Actor-Critic method; the only
difference is the second learning rule where V (s t ) is replaced
by Q(s t , a t ) in QV-learning.
--->

