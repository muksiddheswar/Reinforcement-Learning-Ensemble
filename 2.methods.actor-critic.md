# Actor-Critic

The Actor-Critic (AC) is an temporal difference, on-policy learning algorithm.
Where SARSA and Q-learning only keep track on the Q-function,
Actor-Critic will update both a Critic and Actor function (wiering 2008 [1]).
The Critic function assigns values to the states, 
irrespective of the action chosen by the agent.
The ACtor function will for each action map the states to preference values (wiering 2008).
An experience is defined as the sequence ($(s_t, a_t, r_t, s_{t+1})$).
After each experience,
the Critic and Actor get updated as follows [wiering 2008[11]]:

$$ Critic: V(s_t) := V(s_t) + \beta ( r_t + \gamma V(s_{t+1}) - V(s_t) ) $$

$$ Actor: P(s_t, a_t) := P(s_t, a_t) + \alpha ( r_t + \gamma V(s_{t+1}) - V(s_t) ) $$

Where $\beta$ is the learning rate of the Critic 
and $\alpha$ the learning rate of the Actor.
P-values should not be seen as literal Q-values, 
but instead as preference values.

<!---

wiering 2008 

Actor-Critic. The Actor-Critic (AC) method is an on-policy
algorithm like Sarsa. In contrast to Q-learning and Sarsa, AC
methods keep track of two functions; a Critic that evaluates
states and an Actor that maps states to a preference value
for each action [1]. After an experience (s t , a t , r t , s t+1 ) AC
makes a temporal difference (TD) update to the Critic’s value-
function V :

V (s t ) := V (s t ) + β(r t + γV (s t+1 ) − V (s t ))

where β is the learning rate. AC updates the Actor’s values
P (s t , a t ) as follows:

P (s t , a t ) := P (s t , a t ) + α(r t + γV (s t+1 ) − V (s t ))

where α is the learning rate for the Actor. The P-values should
be seen as preference values and not as exact Q-values.

--->

<!---

wiering 2007

Actor-Critic. Another on-policy algorithm is the Actor-
Critic (AC) method. In contrary to Q-learning and Sarsa, AC
methods keep track of two functions; a Critic that evaluates
states and an Actor that maps states to a preference value
for each action. A number of Actor-Critic methods have been
proposed [1], [4], [11]. Here we will use the Actor-Critic
method described in [11]. After an experience (s t , a t , r t , s t+1 )
AC makes a TD-update to the Critic’s value-function V :

V (s t ) := V (s t ) + β(r t + γV (s t+1 ) − V (s t ))

where β is the learning rate. AC updates the Actor with values
P (s t , a t ) as follows:

P (s t , a t ) := P (s t , a t ) + α(r t + γV (s t+1 ) − V (s t ))

where α is the learning rate for the Actor. The P-values
should be seen as preference values and not as exact Q-values.
Consider a bandit problem with one state and two actions.
Both actions lead to an immediate deterministic reward of 1.
When one action is selected a number of times in a row or
the initial learning rate is 1, the state or V-value and the P-
value for this action converge rapidly to 1. Afterwards the
P-value of the other action can never increase anymore using
AC and will not converge to the underlying Q-value of 1.
A number of Actor-Critic methods have still been proved to
converge to the optimal policy and state value-function for
tabular representations [4].

--->